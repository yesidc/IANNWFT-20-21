{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D0HUwWAEKeM"
   },
   "source": [
    "## Perceptron Logical Gates\n",
    "\n",
    "In this notebook we train a perceptron (just a single neuron) to solve logical gates. \n",
    "\n",
    "That means, e.g. for the AND gate, if we put in a 0 and a 1, the perceptron should output 0. It should only output 1 if the input was (1,1).\n",
    "\n",
    "For the XOR gate, the Perceptron should return 1 if only one of the inputs was 1 and the other one 0 -->  (0,1) or (1,0). For (0,0) and (1,1) it should return 0.\n",
    "\n",
    "And so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7euT0HdQEKeR"
   },
   "outputs": [],
   "source": [
    "# Not using any tf libraries so far...\n",
    "import numpy as np \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8bO7NpCDEKej"
   },
   "outputs": [],
   "source": [
    "# To make sure that you get the implementation right let us first have a look at the data structure.\n",
    "# These are the four possible input pairs of (x1,x2).\n",
    "x = np.array([[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]])\n",
    "# These are possible labels form some logical gates.\n",
    "t_and = np.array([0,0,0,1])#using and, all are false, 0, but one of them\n",
    "t_or = np.array([0,1,1,1])# using or, all of them are true ,1, but one of them\n",
    "t_nand = np.array([1,1,1,0])\n",
    "t_nor = np.array([1,0,0,0])\n",
    "t_xor = np.array([0,1,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_PT3Q_gEKer"
   },
   "source": [
    "#### Implement Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pDWFmomzEKet"
   },
   "outputs": [],
   "source": [
    "# To make you familiar with classes we will implement the perceptron as a class.\n",
    "# Check https://docs.python.org/3/tutorial/classes.html if you need basic help with python classes.\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, input_units):\n",
    "        self.input_units = input_units\n",
    "        self.sigmoid=0\n",
    "        # 1. Initialize random weights and a random bias term. Check 'np.random.randn()'.\n",
    "        self.weights = np.random.randn(input_units)\n",
    "        \n",
    "\n",
    "        self.bias = np.random.randn() #it's a bias per neuron,thus we only need one, this is the code for a percetron\n",
    "        # 2. Define the learning rate as 0.01.\n",
    "        self.alpha = 1\n",
    "        \n",
    "    def forward_step(self, input):\n",
    "        # Perform a perceptron forward step.\n",
    "        # 1. Calculate the drive. You can use @ as a matrix multiplication command.\n",
    "        weighted_sum =  self.weights @ input + self.bias \n",
    "        print(weighted_sum)\n",
    "        #implementing sigmoid as an activation function.\n",
    "        self.sigmoid=1/(1+math.exp(-weighted_sum))\n",
    "        \n",
    "        return self.sigmoid\n",
    "        \n",
    "        \"\"\"\"\n",
    "        # 2. Return a 1 or a 0, depending on whether the perceptron surpassed the threshold. \n",
    "        # You can use 'int(...)' to make an integer out of a boolean.\n",
    "        #the inputs of the neuron would be the activations in the l-1 layer of that neuron\n",
    "        self.input_activation= input\n",
    "        return int(weighted_sum >= 0)\n",
    "        \"\"\"\n",
    "    def sigmoid_prime(self):\n",
    "        derivative_sigmoid=self.sigmoid*(1-self.sigmoid)\n",
    "        return derivative_sigmoid\n",
    "    \n",
    "    def update(self,delta,activation):\n",
    "        #compute the error (which is just a scalar) times all the activations, which is \n",
    "        #a vector.are the activations in the layer l-1 just the inputs of the \n",
    "        #perceptron???\n",
    "        #RECALL! every layer perceptron has as many weights as activations the previous\n",
    "        #layer has\n",
    "        print('hi')\n",
    "        print(activation)\n",
    "        print(delta)\n",
    "      \n",
    "        gradient_weights= delta*activation\n",
    "        new_weights= self.weights-(self.alpha*gradient_weights)\n",
    "        #update weights\n",
    "        self.weights=new_weights\n",
    "    \n",
    "    \n",
    "    def training_step(self, input, label):\n",
    "        # Perform a whole training step (including the forward step).\n",
    "        # 1. Forward step.\n",
    "        prediction = self.forward_step(input)\n",
    "        # 2. Calculate the weight updates.\n",
    "        delta_weights = self.alpha * (label - prediction) * input\n",
    "        # 3. Calculate the bias update. We consider 1 as the \"input\" to the bias, so that we can treat it just like the other weights.\n",
    "        delta_bias = self.alpha * (label - prediction) * 1\n",
    "        # 4. Update weights and bias.\n",
    "        self.weights += delta_weights\n",
    "        self.bias += delta_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9820137900379085\n",
      "0.017662706213291107\n"
     ]
    }
   ],
   "source": [
    "#DON'T PAY ATTENTION TO THIS\n",
    "weighted_sum=4\n",
    "empty_a= np.array([])\n",
    "for i in range(3):\n",
    "    empty_a=np.append(empty_a,Perceptron(2))\n",
    "sigmoid=1/(1+math.exp(-weighted_sum))\n",
    "derivative_sigmoid=sigmoid*(1-sigmoid)\n",
    "print(sigmoid)\n",
    "print(derivative_sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.12243375 -1.58766422]\n",
      "[0.72325381 0.15081959]\n",
      "[-0.85217053 -0.28496953]\n"
     ]
    }
   ],
   "source": [
    "#DON'T PAY ATTENTION TO THIS\n",
    "for j in range(3):\n",
    "    print(empty_a[j].weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, hidden_layer, num_perceptrons,input):\n",
    "        #FOWARD IMPLIMENTATION\n",
    "        #label takes in the desired output for some specific input\n",
    "    \n",
    "        #self.label=label\n",
    "        self.input=input\n",
    "        self.hidden_layer=hidden_layer\n",
    "        self.num_perceptrons=num_perceptrons\n",
    "        #we need 8 weights, since I have 2 inputs and 4 perceptrons, I have to call forward_step\n",
    "        #4 times, so that I can compute the the activation for each perceptron\n",
    "        #this array stores the activation of the neurons\n",
    "        self.activation=np.array([])\n",
    "        #this array stores the weights for each neuron\n",
    "        self.weights_per_percep=np.array([])\n",
    "        #this loop creates as many Perceptron objects as needed, and they're stored in\n",
    "        #weights_per_percep. These are all the perceptrons of the hidden layer\n",
    "        for j in range(4):\n",
    "            #remember that the number Perceptron(2) pass to the Perceptron class\n",
    "            #indicates the numbers of weights each perceptron needs\n",
    "            self.weights_per_percep=np.append(self.weights_per_percep,Perceptron(2))\n",
    "        #this loop takes each object from the weights_per_percep array, and calls the\n",
    "        #function foward_step\n",
    "        for i in range(4):\n",
    "            #calculate the activations for each perceptron \n",
    "            self.activation=np.append(self.activation,self.weights_per_percep[i].forward_step(input))\n",
    "        print(self.activation)\n",
    "        #I am using this len(activation), because for the output layer I need as many \n",
    "        #weights as activations the previus layer had. \n",
    "        #output_perceptron_weights is the last perceptron, and as such it is an object\n",
    "        self.output_perceptron_weights=Perceptron(len(self.activation))\n",
    "        self.activation_output_percep= self.output_perceptron_weights.forward_step(self.activation)\n",
    "        print(self.activation_output_percep)\n",
    "    def foward_mlp(input_foward):\n",
    "        for i in rage(len(self.weights_per_percep)):\n",
    "            self.weights_per_percep.forward_step(input_foward)\n",
    "\n",
    "   \n",
    "    def backprop_step(self,labels,inputs):\n",
    "        print(self.activation)\n",
    "        #to compute delta you need the derivative of the sigmoid, this one must be\n",
    "        #calculated in Peceptron class function foward_step, since that function has\n",
    "        #access to the net input needed to compute the derivative of the sigmoid\n",
    "        delta_output_layer=(labels - self.activation_output_percep)*self.output_perceptron_weights.sigmoid_prime()\n",
    "        print(delta_output_layer)\n",
    "        #pass the error to the update function, the function has to be called on the object\n",
    "        #output_perceptron_weights, because I want to update the weights that belong to that\n",
    "        #specific object.\n",
    "        self.output_perceptron_weights.update(delta_output_layer,self.activation)\n",
    "        \n",
    "        #this code below computes the error for the perceptrons in the hidden layer\n",
    "        \n",
    "        for j in range(len( self.weights_per_percep)):\n",
    "            perceptron_weight=self.weights_per_percep[j].weights[j]\n",
    "            print('this is the perceptron weight: '+str(perceptron_weight))\n",
    "            derivative=self.weights_per_percep[j].sigmoid_prime()\n",
    "            print(derivative)\n",
    "            delta_hidden_layer=delta_output_layer*perceptron_weight*derivative\n",
    "            print('this is the error of the hidden layer'+str(delta_hidden_layer))\n",
    "            self.weights_per_percep[j].update(delta_hidden_layer,inputs)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9146862273385623\n",
      "3.5103717823961147\n",
      "0.8609235369208714\n",
      "-3.2065946268032977\n",
      "[0.87154471 0.97098144 0.70285357 0.03891831]\n",
      "1.5894942886707097\n",
      "0.8305449411292797\n"
     ]
    }
   ],
   "source": [
    "input_test=np.array([4,5])\n",
    "t=MLP(1,1,input_test)\n",
    "#t.backprop_step([[1]],input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "# Now let's train the perceptron.\n",
    "\n",
    "# Choose which target you want to train on.\n",
    "t = t_and\n",
    "# Initialize a perceptron. A new object is created using the clas Perceptron\n",
    "multilayer = MLP(1,1,)\n",
    "# Initialize lists to store steps and performance.\n",
    "steps = []\n",
    "accuracies = []\n",
    "\n",
    "# We train for 500 steps.\n",
    "for i in range(500):\n",
    "    steps.append(i)\n",
    "    \n",
    "    # 1. Draw a random sample from x and the corresponding t. Check 'np.random.randint'.\n",
    "    index = np.random.randint(len(x))\n",
    "    #the index of the sample is the same as the index of the labeel. \n",
    "    sample = x[index]\n",
    "    label = t[index]\n",
    "    # 2. Perform a training step.\n",
    "    perceptron.training_step(sample, label)\n",
    "    \n",
    "    # Calculate the performance over all four possible inputs.\n",
    "    accuracy_sum = 0\n",
    "    for k in range(len(x)):\n",
    "        #bear in mind that the forward_step function runs with the newly updated weigths\n",
    "        #that's why it makes to calculate the accuracy. So we're taking all the \n",
    "        #possible inputs, and then we make predictions with these newly updated weights\n",
    "        output = perceptron.forward_step(x[k])   # feed each input to the perceptron (only foward step!)\n",
    "        accuracy_sum += int(output == t[k])  # this turns true if the perceptron gave the right output --> then adds 1 \n",
    "    accuracy = accuracy_sum / 4                  # we divide through all the training examples\n",
    "    accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7cSOPUIfjzD"
   },
   "source": [
    "Note that we are **adding** the weights/biases, because we are using the Perceptron Learning rule and **not Gradient Descent**!\n",
    "\n",
    "#### Perceptron Learning Rule Recap:\n",
    "$$w_i^{new} = w_i^{old} + \\Delta w_i  $$\n",
    "$$\\Delta w_i = \\alpha*  (t-y) *x_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrnCRFB0EKe1"
   },
   "source": [
    "#### Train on AND gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "rZ0Bll4-EKe2"
   },
   "outputs": [],
   "source": [
    "#x = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "# Now let's train the perceptron.\n",
    "\n",
    "# Choose which target you want to train on.\n",
    "t = t_and\n",
    "# Initialize a perceptron. A new object is created using the clas Perceptron\n",
    "perceptron = Perceptron(2)\n",
    "# Initialize lists to store steps and performance.\n",
    "steps = []\n",
    "accuracies = []\n",
    "\n",
    "# We train for 500 steps.\n",
    "for i in range(500):\n",
    "    steps.append(i)\n",
    "    \n",
    "    # 1. Draw a random sample from x and the corresponding t. Check 'np.random.randint'.\n",
    "    index = np.random.randint(len(x))\n",
    "    #the index of the sample is the same as the index of the labeel. \n",
    "    sample = x[index]\n",
    "    label = t[index]\n",
    "    # 2. Perform a training step.\n",
    "    perceptron.training_step(sample, label)\n",
    "    \n",
    "    # Calculate the performance over all four possible inputs.\n",
    "    accuracy_sum = 0\n",
    "    for k in range(len(x)):\n",
    "        #bear in mind that the forward_step function runs with the newly updated weigths\n",
    "        #that's why it makes to calculate the accuracy. So we're taking all the \n",
    "        #possible inputs, and then we make predictions with these newly updated weights\n",
    "        output = perceptron.forward_step(x[k])   # feed each input to the perceptron (only foward step!)\n",
    "        accuracy_sum += int(output == t[k])  # this turns true if the perceptron gave the right output --> then adds 1 \n",
    "    accuracy = accuracy_sum / 4                  # we divide through all the training examples\n",
    "    accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88m7nylyqaly"
   },
   "source": [
    "#### Let's give it a try:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4bZnF_tchiQ",
    "outputId": "47e73964-b082-4ba8-f06f-6f9b25a106cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0] \t True Label: 0 \t Perceptron's Prediction: 0\n",
      "Input: [0 1] \t True Label: 0 \t Perceptron's Prediction: 0\n",
      "Input: [1 0] \t True Label: 0 \t Perceptron's Prediction: 0\n",
      "Input: [1 1] \t True Label: 1 \t Perceptron's Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(x)):\n",
    "  output = perceptron.forward_step(x[n])\n",
    "  print(\"Input: %s \\t True Label: %d \\t Perceptron's Prediction: %d\" % (np.array2string(x[n]), t[n], output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8vk4Qe2EKe9"
   },
   "source": [
    "#### Visualize training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "QmodvcrUEKfA",
    "outputId": "8f1c00b4-f706-4fdc-8fdf-1967dab1d114"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lastly let's plot the training progress.\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(steps, accuracies)\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.show()\n",
    "# this graphs show how the accuracy improves as the interations increase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oU3aBtCpEKfF"
   },
   "source": [
    "#### Train on XOR gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "FPgctnhDEKfH"
   },
   "outputs": [],
   "source": [
    "# Initialize a perceptron.\n",
    "perceptron = Perceptron(2)\n",
    "# Initialize list to store performance.\n",
    "accuracies = []\n",
    "steps =[]\n",
    "\n",
    "# We train for 500 steps.\n",
    "for i in range(500):\n",
    "    steps.append(i)\n",
    "    \n",
    "    # 1. Draw a random sample from x and the corresponding t. Check 'np.random.randint'.\n",
    "    index = np.random.randint(len(x))\n",
    "    sample = x[index]\n",
    "    label = t_xor[index]\n",
    "    # 2. Perform a training step.\n",
    "    perceptron.training_step(sample, label)\n",
    "    \n",
    "    # Calculate the performance over all four possible inputs.\n",
    "    accuracy_sum = 0\n",
    "    for k in range(4):\n",
    "        output = perceptron.forward_step(x[k])   # feed each input to the perceptron (only foward step!)\n",
    "        accuracy_sum += int(output == t_xor[k])  # this turns true if the perceptron gave the right output --> then adds 1 \n",
    "    accuracy = accuracy_sum / 4                  # we divide through all the training examples\n",
    "    accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRpkOcWoEKfP"
   },
   "source": [
    "#### Visualize training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "ttCxt0aGEKfS",
    "outputId": "f764834a-2298-4baa-8134-dcd92da2bbc2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(steps, accuracies)\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Flipped_classroom02.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
